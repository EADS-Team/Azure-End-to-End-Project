 dbutils.fs.ls('mnt/silver/SalesLT')
 dbutils.fs.ls('mnt/gold/')
 df = spark.read.format('delta').load('/mnt/silver/SalesLT/Address/')
    display(df)
 from pyspark.sql.functions import col
 
def rename_columns_to_snake_case(df):
    """
    Convert column names from PascalCase or camelCase to snake_case in a PySpark DataFrame:
 
    Args:
        df (DataFrame) : The input  DataFrame with columns to be renamed.
 
    Returns :
        DataFrame : A new DataFrame with column names converted to snake_case
    """
    # Get the list of column names
    column_names = df.columns
     
    # Dictionary to hold old and new column name mapping
    rename_map = {}
 
    for old_col_name in column_names:
        # Convert column name from PascalCase or camelCase to snake_case
        new_col_name = "".join([
            "_" + char.lower() if (
                char.isupper()            # check if the current character is upper case
                and idx > 0               # Ensure  it's not the first character
                and not old_col_name[idx - 1].isupper() # Ensure the previous character is not uppercase
            ) else char.lower()     # Convert character to lowercase
            for idx, char in enumerate(old_col_name)
            ]).lstrip("_")  # Remove  any leading underscore
       
        # Avoid renaming to an existing column name
        if new_col_name in rename_map.values():
            raise ValueError(f"Duplicate column name found after renaming : '{new_col_name}'")
 
        # Map the old column name to the new column name
        rename_map[old_col_name] = new_col_name
 
    # Rename column using the mapping
    for old_col_name, new_col_name in rename_map.items():
        df = df.withColumnRenamed(old_col_name, new_col_name)
 
    return df
 df = rename_columns_to_snake_case(df)
    display(df)
 table_name_temp = []
 
for i in dbutils.fs.ls('mnt/silver/SalesLT/'):
    table_name_temp.append(i)
 table_name_temp
 table_name = []
 
for i in dbutils.fs.ls('mnt/silver/SalesLT/'):
    table_name.append(i.name.split('/')[0])
 table_name
 for name in table_name:
    path = '/mnt/silver/SalesLT/' + name
    print(path)
    df = spark.read.format('delta').load(path)
 
    df = rename_columns_to_snake_case(df)
 
    output_path = '/mnt/gold/SalesLT/' + name + '/'
    df.write.format('delta').mode('overwrite').save(output_path)
 display(df)
